// Kernel Heap Allocator
// Simple bump allocator with free list for deallocation

// ============================================================
// Constants
// ============================================================

const PAGE_SIZE: u64 = 4096;
const HEAP_INITIAL_PAGES: u64 = 16;   // 64KB initial heap
const HEAP_MAX_PAGES: u64 = 16384;    // 64MB max heap
const MIN_ALLOC_SIZE: u64 = 32;       // Minimum allocation size
const ALIGNMENT: u64 = 16;            // Allocation alignment

// ============================================================
// Free List Entry
// ============================================================

// Free block header
#[repr(C)]
struct FreeBlock {
    size: u64,       // Size of this free block (including header)
    next: u64,       // Pointer to next free block (0 if none)
}

// Allocation header (prepended to every allocation)
#[repr(C)]
struct AllocHeader {
    size: u64,       // Size of allocation (including header)
    magic: u64,      // Magic number for validation
}

const ALLOC_MAGIC: u64 = 0xDEADBEEF_CAFEBABE;

// ============================================================
// State
// ============================================================

static mut HEAP_START: u64 = 0;        // Start of heap region
static mut HEAP_END: u64 = 0;          // Current end of heap
static mut HEAP_MAX: u64 = 0;          // Maximum heap address
static mut HEAP_BUMP: u64 = 0;         // Current bump pointer
static mut FREE_LIST: u64 = 0;         // Head of free list
static mut TOTAL_ALLOCATED: u64 = 0;   // Total bytes allocated
static mut ALLOC_COUNT: u64 = 0;       // Number of allocations
static mut HEAP_PAGES: u64 = 0;        // Current number of heap pages

// Function pointers for dynamic heap expansion
static mut PAGE_ALLOC_FN: u64 = 0;     // fn() -> u64 stored as u64
static mut MAP_PAGE_FN: u64 = 0;       // fn(u64, u64, u64, fn() -> u64) -> bool stored as u64

// ============================================================
// Initialization
// ============================================================

// Initialize heap with given region
// page_alloc: function to allocate physical pages
// map_page: function to map pages
pub fn init(page_alloc: fn() -> u64, map_fn: fn(u64, u64, u64, fn() -> u64) -> bool) -> bool {
    unsafe {
        // Store function pointers for later use in extend_heap
        PAGE_ALLOC_FN = page_alloc as u64;
        MAP_PAGE_FN = map_fn as u64;

        // Use a fixed virtual address range for heap (16MB - 80MB for 64MB heap)
        HEAP_START = 0x1000000;   // 16MB
        HEAP_MAX = 0x5000000;     // 80MB (allows 64MB heap)
        HEAP_END = HEAP_START;
        HEAP_BUMP = HEAP_START;
        FREE_LIST = 0;
        TOTAL_ALLOCATED = 0;
        ALLOC_COUNT = 0;
        HEAP_PAGES = 0;

        // Map initial pages
        let mut addr = HEAP_START;
        let mut i: u64 = 0;
        while i < HEAP_INITIAL_PAGES {
            let phys = page_alloc();
            if phys == 0 {
                return false;
            }

            // Map with write permission (PAGE_PRESENT | PAGE_WRITE = 3)
            if !map_fn(addr, phys, 3, page_alloc) {
                return false;
            }

            addr = addr + PAGE_SIZE;
            i = i + 1;
        }

        HEAP_END = addr;
        HEAP_PAGES = HEAP_INITIAL_PAGES;
        true
    }
}

// ============================================================
// Allocation
// ============================================================

// Align size up to alignment boundary
fn align_up(size: u64, align: u64) -> u64 {
    (size + align - 1) & (!(align - 1))
}

// Allocate memory from heap
pub fn alloc(size: u64) -> *mut u8 {
    if size == 0 {
        return 0 as *mut u8;
    }

    unsafe {
        // Calculate actual size needed (header + aligned size)
        let header_size: u64 = 16;  // size_of::<AllocHeader>()
        let actual_size = align_up(size + header_size, ALIGNMENT);
        let min_size = if actual_size < MIN_ALLOC_SIZE { MIN_ALLOC_SIZE } else { actual_size };

        // Try to find a suitable free block first
        let block = find_free_block(min_size);
        if block != 0 {
            // Found a free block, use it
            let header = block as *mut AllocHeader;
            let header_size_ptr = header as *mut u64;
            let header_magic_ptr = (block + 8) as *mut u64;

            volatile_write_u64(header_size_ptr, min_size);
            volatile_write_u64(header_magic_ptr, ALLOC_MAGIC);

            TOTAL_ALLOCATED = TOTAL_ALLOCATED + min_size;
            ALLOC_COUNT = ALLOC_COUNT + 1;

            return (block + 16) as *mut u8;
        }

        // No suitable free block, bump allocate
        if HEAP_BUMP + min_size > HEAP_END {
            // Need to extend heap
            if !extend_heap(min_size) {
                return 0 as *mut u8;  // Out of memory
            }
        }

        let block_addr = HEAP_BUMP;
        HEAP_BUMP = HEAP_BUMP + min_size;

        // Write allocation header
        let header_size_ptr = block_addr as *mut u64;
        let header_magic_ptr = (block_addr + 8) as *mut u64;

        volatile_write_u64(header_size_ptr, min_size);
        volatile_write_u64(header_magic_ptr, ALLOC_MAGIC);

        TOTAL_ALLOCATED = TOTAL_ALLOCATED + min_size;
        ALLOC_COUNT = ALLOC_COUNT + 1;

        (block_addr + 16) as *mut u8
    }
}

// Allocate zeroed memory
pub fn alloc_zeroed(size: u64) -> *mut u8 {
    let ptr = alloc(size);
    if ptr as u64 != 0 {
        // Zero the memory
        let mut i: u64 = 0;
        while i < size {
            unsafe {
                let byte_ptr = (ptr as u64 + i) as *mut u8;
                volatile_write_u8(byte_ptr, 0);
            }
            i = i + 1;
        }
    }
    ptr
}

// Find a free block of at least the given size
fn find_free_block(size: u64) -> u64 {
    unsafe {
        let mut prev_ptr: u64 = 0;
        let mut current = FREE_LIST;

        while current != 0 {
            let block_size_ptr = current as *u64;
            let block_next_ptr = (current + 8) as *u64;

            let block_size = volatile_read_u64(block_size_ptr);
            let block_next = volatile_read_u64(block_next_ptr);

            if block_size >= size {
                // Found suitable block, remove from free list
                if prev_ptr == 0 {
                    FREE_LIST = block_next;
                } else {
                    let prev_next_ptr = (prev_ptr + 8) as *mut u64;
                    volatile_write_u64(prev_next_ptr, block_next);
                }
                return current;
            }

            prev_ptr = current;
            current = block_next;
        }

        0  // No suitable block found
    }
}

// Extend heap by allocating more pages
fn extend_heap(min_size: u64) -> bool {
    unsafe {
        // Check if function pointers are initialized
        if PAGE_ALLOC_FN == 0 || MAP_PAGE_FN == 0 {
            return false;
        }

        // Calculate pages needed (at least 16 pages = 64KB at a time for efficiency)
        let min_extend = if min_size < PAGE_SIZE * 16 { PAGE_SIZE * 16 } else { min_size };
        let needed = align_up(min_extend, PAGE_SIZE);
        let pages_needed = needed / PAGE_SIZE;

        // Check we won't exceed maximum
        if HEAP_END + needed > HEAP_MAX {
            return false;
        }

        // Check we won't exceed max pages
        if HEAP_PAGES + pages_needed > HEAP_MAX_PAGES {
            return false;
        }

        // Cast function pointers back to functions
        let page_alloc: fn() -> u64 = core::mem::transmute(PAGE_ALLOC_FN);
        let map_fn: fn(u64, u64, u64, fn() -> u64) -> bool = core::mem::transmute(MAP_PAGE_FN);

        // Allocate and map pages
        let mut i: u64 = 0;
        while i < pages_needed {
            let phys = page_alloc();
            if phys == 0 {
                return false;  // Out of physical memory
            }

            let virt = HEAP_END + (i * PAGE_SIZE);
            // Map with write permission (PAGE_PRESENT | PAGE_WRITE = 3)
            if !map_fn(virt, phys, 3, page_alloc) {
                return false;  // Failed to map page
            }

            i = i + 1;
        }

        HEAP_END = HEAP_END + needed;
        HEAP_PAGES = HEAP_PAGES + pages_needed;
        true
    }
}

// ============================================================
// Deallocation
// ============================================================

// Free allocated memory
pub fn free(ptr: *mut u8) {
    if ptr as u64 == 0 {
        return;
    }

    unsafe {
        let block_addr = (ptr as u64) - 16;

        // Validate magic number
        let magic_ptr = (block_addr + 8) as *u64;
        let magic = volatile_read_u64(magic_ptr);

        if magic != ALLOC_MAGIC {
            // Invalid or double free
            return;
        }

        // Get block size
        let size_ptr = block_addr as *u64;
        let block_size = volatile_read_u64(size_ptr);

        // Clear magic to prevent double-free
        let magic_mut_ptr = (block_addr + 8) as *mut u64;
        volatile_write_u64(magic_mut_ptr, 0);

        // Add to free list
        let next_ptr = (block_addr + 8) as *mut u64;
        volatile_write_u64(next_ptr, FREE_LIST);
        FREE_LIST = block_addr;

        TOTAL_ALLOCATED = TOTAL_ALLOCATED - block_size;
        if ALLOC_COUNT > 0 {
            ALLOC_COUNT = ALLOC_COUNT - 1;
        }
    }
}

// ============================================================
// Statistics
// ============================================================

pub fn get_total_allocated() -> u64 {
    unsafe { TOTAL_ALLOCATED }
}

pub fn get_alloc_count() -> u64 {
    unsafe { ALLOC_COUNT }
}

pub fn get_heap_size() -> u64 {
    unsafe { HEAP_END - HEAP_START }
}

pub fn get_heap_used() -> u64 {
    unsafe { HEAP_BUMP - HEAP_START }
}

pub fn get_heap_free() -> u64 {
    unsafe { HEAP_END - HEAP_BUMP }
}

pub fn get_heap_pages() -> u64 {
    unsafe { HEAP_PAGES }
}

pub fn get_heap_max() -> u64 {
    unsafe { HEAP_MAX - HEAP_START }
}

// Reallocate memory block to new size
pub fn realloc(ptr: *mut u8, new_size: u64) -> *mut u8 {
    if ptr as u64 == 0 {
        return alloc(new_size);
    }

    if new_size == 0 {
        free(ptr);
        return 0 as *mut u8;
    }

    unsafe {
        let block_addr = (ptr as u64) - 16;

        // Validate magic number
        let magic_ptr = (block_addr + 8) as *u64;
        let magic = volatile_read_u64(magic_ptr);

        if magic != ALLOC_MAGIC {
            return 0 as *mut u8;  // Invalid pointer
        }

        // Get old block size
        let size_ptr = block_addr as *u64;
        let old_size = volatile_read_u64(size_ptr);
        let old_data_size = old_size - 16;  // Subtract header

        // If new size fits in old block, just return same pointer
        let header_size: u64 = 16;
        let actual_new_size = align_up(new_size + header_size, ALIGNMENT);
        if actual_new_size <= old_size {
            return ptr;
        }

        // Allocate new block
        let new_ptr = alloc(new_size);
        if new_ptr as u64 == 0 {
            return 0 as *mut u8;
        }

        // Copy old data to new block
        let copy_size = if old_data_size < new_size { old_data_size } else { new_size };
        let mut i: u64 = 0;
        while i < copy_size {
            let src = (ptr as u64 + i) as *u8;
            let dst = (new_ptr as u64 + i) as *mut u8;
            volatile_write_u8(dst, volatile_read_u8(src));
            i = i + 1;
        }

        // Free old block
        free(ptr);

        new_ptr
    }
}
